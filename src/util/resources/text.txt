word2vec is a group of related models that are used to produce word embeddings
these models are shallow two-layer neural networks that are trained to reconstruct linguistic contexts of words
word2vec takes as its input a large corpus of text and produces a high-dimensional space typically of several hundred dimensions with each unique word in the corpus being assigned a corresponding vector in the space
word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space
word2vec was created by a team of researchers led by tomas mikolov at google
the algorithm has been subsequently analysed and explained by other researchers and a bayesian version of the algorithm is proposed as well
embedding vectors created using the word2vec algorithm have many advantages compared to earlier algorithms like latent semantic analysis
word2vec is a two-layer neural net that processes text 
its input is a text corpus and its output is a set of vectors feature vectors for words in that corpus
while word2vec is not a deep neural network it turns text into a numerical form that deep nets can understand 
deeplearning4j implements a distributed form of word2vec for java and scala which works on spark with gpus
word2vec applications extend beyond parsing sentences in the wild
it can be applied just as well to genes code likes playlists social media graphs and other verbal or symbolic series in which patterns may be discerned
why
because words are simply discrete states like the other data mentioned above and we are simply looking for the transitional probabilities between those states the likelihood that they will co-occur 
so gene2vec like2vec and follower2vec are all possible
with that in mind the tutorial below will help you understand how to create neural embeddings for any group of discrete and co-occurring states
the purpose and usefulness of word2vec is to group the vectors of similar words together in vectorspace 
that is it detects similarities mathematically 
word2vec creates vectors that are distributed numerical representations of word features features such as the context of individual words
it does so without human intervention